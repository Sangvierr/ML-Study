{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2024.04.07] 이상혁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개요\n",
    "- Support Vector Machine(SVM)은 분류 성능이 뛰어나다고 알려져 있다.\n",
    "- Logistic Regression과 같은 분류 모델은 조건부 확률을 추정해서 파라미터를 결정하지만, SVM은 최적화를 통해서 분류를 진행한다는 점이 특이하다.\n",
    "- 실제로 다양한 딥러닝 모델들의 성능을 벤치마크할 때 SVM이 비교 모델로 등장하는 경우가 많다.\n",
    "- SVM을 이해하기 위해 다음 4가지 과정을 살펴보면서 모델의 원리와 활용을 알아보려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1️⃣ Seperating Hyperplanes\n",
    "\n",
    "    2️⃣ Maximal Margin Classifier\n",
    "    \n",
    "    3️⃣ Support Vector Classifier\n",
    "    \n",
    "    4️⃣ Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Seperating Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM은 데이터를 분류하기 위해 **분류 경계**를 찾는다. 그리고 이 분류 경계를 **Hyperplane(초평면)**이라고 부른다. 데이터가 2차원이라면 초평면은 직선이 된다. 그리고 데이터가 3차원이라면 초평면은 면이 된다. 그리고 4차원 이상부터는 이 분류 경계를 초평면이라고 부르는 것이다. 일반적으로 <span style=\"background-color:#fff5b1\"> p-차원의 초평면</span>은 다음과 같이 표현할 수 있다.\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1  + \\beta_2 X_2 + \\cdots + \\beta_p X_p = 0 $$\n",
    "\n",
    "이는 회귀식의 형태로 데이터를 바탕으로 최적의 회귀계수를 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 초평면이 0이 되는 점을 기준으로 0 이상인 부분과 0 미만인 부분으로 분류를 진행하는 것이다.\n",
    "\n",
    "예를 들어 4차원인 데이터가 있다고 하자. <span style=\"color:gray\"> (변수가 4개라는 말이다.)</span> 그러면 분류 경계면은 다음과 같다. $$\\beta_0 + \\beta_1 X_1  + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 = 0$$\n",
    "\n",
    "그리고 $x_i = (x_{i1}, x_{i2}, x_{i3}, x_{i4})^T$를 분류 경계면에 대입하여 0 기준으로 이상인지, 미만인지를 확인한다. 0을 기준으로 이상이면 1, 미만이면 -1로 데이터를 분류할 수 있게 된다. \n",
    "\n",
    "이처럼 초평면을 기준으로 데이터를 분류할 수 있다고 해서 Seperating Hyperplanes라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. Adavantages & Disadavantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤩 Adavantages \n",
    "1. 확률 추정이 없이 초평면이라는 분류 경계를 바탕으로 데이터를 쉽게 분류할 수 있다.\n",
    "\n",
    "😱 Disadavantages \n",
    "1. 데이터가 seperable한 경우만 다룬다. 분류되는 데이터들 간에 overlapping이 있으면 분류를 할 수 없다.\n",
    "2. 초평면은 유일하지 않다. 데이터를 바탕으로 회귀계수를 찾기 때문에 여러 개의 초평면이 존재할 수 있다. 이는 데이터가 seperable한 경우만 다뤄서 그렇다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Maximal Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. How to Maximaize the Margin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperplanes만 사용해서 데이터를 분류하는 벙법은 분류 경계가 유일하지 않다는 단점이 있다. 이를 해결하기 위해 **Maximal Margin Classifier**라는 방법을 사용할 수 있다.\n",
    "\n",
    "이는 <span style=\"background-color:#fff5b1\"> Margin을 최대화하는 Hyperplane</span>을 찾는 것으로, Hyperplane에 제약조건이 생기기 때문에 😎**유일한 분류 경계**를 찾을 수 있게 된다. 그럼 Margin은 무엇일까?\n",
    "\n",
    "**Margin**은 데이터와 초평면 사이에 수직으로 내린 거리 중 가장 최소의 거리를 말한다. 예를 들어 2차원 데이터를 생각해보자. 데이터를 잘 분류하는 초평면인 직선을 찾기 위해서는 다음의 2가지 조건을 생각하면 된다.\n",
    "\n",
    "    1️⃣ 데이터에서 직선으로 수직으로 내린 선분의 거리가 가장 작은 직선을 찾는다.\n",
    "\n",
    "    2️⃣ 이 선분의 길이가 가장 Maximize되는 직선(초평면)이 데이터를 가장 잘 분류하는 분류 경계가 된다.\n",
    "\n",
    "이렇게 위 두 가지 조건을 바탕으로 최적화해 나가면서 찾은 초평면과 Margin은 데이터를 가장 잘 분류하는 경계라고 할 수 있다. 그리고 Margin을 최대화하는 분류 경계는 하나 뿐이기 때문에 분류 경계도 유일해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. Support Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 여기서 Support Vector라는 개념이 나오게 된다. 데이터에서 초평면에 수직으로 선분을 내리며 Margin의 크기를 정하게 된다면, 최종적으로 Margin을 설정하는데 직접적으로 영향을 준 데이터 포인트가 있을 것이다. \n",
    "\n",
    "이 데이터 포인트를 **Support Vector**라고 부른다. Support Vector는 Margin 위에 걸쳐있는 데이터 포인트라고 이해하면 쉬울 것 같다.\n",
    "\n",
    "그리고 Maximal Margin Classifier의 분류 경계를 정하는 데에는 모든 Training 데이터가 필요하지는 않다. 왜냐하면 Support Vector만 Margin을 최적화하는데 영향을 주기 때문이다. \n",
    "\n",
    "그럼 이 분류 경계는 어떻게 추정하는가? 다음 수식을 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ max_{\\beta_p, \\beta} M, \\texttt{subject to} \\ \\beta^T \\beta = 1 \\ \\texttt{and} \\ y_i(\\beta_0 + x_{i}^T\\beta) \\geq M $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 수식의 의미를 하나씩 생각해보자. \n",
    "\n",
    "▶ $M$은 Margin을 뜻한다. 이제 우리는 Margin을 최대화하는 분류 경계의 **회귀 계수**를 찾으면 된다. \n",
    "\n",
    "▶ $\\beta^T \\beta = 1$은 unique한 분류 경게를 찾기 위한 **제약 조건**이다.\n",
    "\n",
    "▶ $y_i(\\beta_0 + x_{i}^T\\beta) \\geq M$은 데이터로부터 초평면까지의 **수직 거리**를 의미한다. 각 데이터 포인트에 대한 해석을 거리로 만들어준 것이다. 이는 모든 데이터 포인트들의 거리가 $M$보다 크다는 것으로 Margin 상이나 Margin 밖에 데이터 포인트들이 존재한다는 것이다. <span style=\"color:gray\"> (물론 여기서 Margin 위에 있는 데이터 포인트는 Support Vector인 점은 까먹지 말자.)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. Adavantages & Disadavantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤩 Adavantages \n",
    "1. 거리를 바탕으로 초평면을 최적화하기 때문에 분류 경게가 유일하다.\n",
    "\n",
    "😱 Disadavantages \n",
    "1. 여전히 데이터가 seperable한 경우만 다룬다. 분류되는 데이터들 간에 overlapping이 있으면 분류를 할 수 없다.\n",
    "2. 즉, **Error**를 단 하나도 허용하지 않기 때문에 범용적인 분류 모델이 되기 어렵다. <span style=\"color:gray\"> (Data-specific한 분류 경계가 될 가능성이 높다.)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://m.blog.naver.com/jaehong7719/221928401297\n",
    "\n",
    "\n",
    "https://velog.io/@im-shung/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4-%EA%B8%80-%EC%83%89%EC%83%81%ED%98%95%EA%B4%91%ED%8E%9C\n",
    "\n",
    "\n",
    "https://kr.piliapp.com/emoji/list/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
